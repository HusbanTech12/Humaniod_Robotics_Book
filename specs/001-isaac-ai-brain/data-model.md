# Data Model: NVIDIA Isaac AI-Robot Brain

## Entity Definitions

### HumanoidRobotModel
- **Description**: Represents the physical humanoid robot with articulated joints, sensors, and kinematic constraints
- **Attributes**:
  - id: Unique identifier for the robot instance
  - name: Descriptive name of the robot model
  - urdf_path: Path to the URDF file defining the robot structure
  - joint_configurations: Map of joint names to position/velocity limits
  - sensor_configurations: List of sensors attached to the robot
  - kinematic_constraints: Bipedal locomotion specific constraints
  - mass_properties: Center of mass, moments of inertia
- **Relationships**:
  - Has many SensorData instances
  - Has many RobotState instances
  - Associated with one SimulationEnvironment

### SensorData
- **Description**: Raw sensor data collected from the humanoid robot's sensor suite
- **Attributes**:
  - id: Unique identifier for the sensor data instance
  - robot_id: Reference to the HumanoidRobotModel
  - sensor_type: Type of sensor (RGB_CAMERA, DEPTH_CAMERA, LIDAR, IMU)
  - timestamp: Time when the data was collected
  - data_payload: The actual sensor data (image, point cloud, acceleration, etc.)
  - frame_id: Coordinate frame in which the data is expressed
  - sensor_position: 3D position of the sensor on the robot
  - sensor_orientation: Orientation of the sensor relative to robot
- **Relationships**:
  - Belongs to one HumanoidRobotModel
  - Associated with PerceptionOutput (processed data)

### PerceptionOutput
- **Description**: Processed perception data generated by Isaac ROS perception pipelines
- **Attributes**:
  - id: Unique identifier for the perception output
  - input_sensor_data_ids: List of sensor data IDs used as input
  - output_type: Type of perception output (OBJECT_DETECTION, SLAM_MAP, FUSED_DATA)
  - timestamp: Time when the perception was processed
  - confidence_score: Confidence level of the perception output
  - data_payload: Processed perception data (object poses, map, etc.)
  - coordinate_frame: Reference frame of the output data
- **Relationships**:
  - References multiple SensorData instances
  - Associated with NavigationGoal (for path planning)
  - Associated with LearningEpisode (for training data)

### NavigationGoal
- **Description**: Goal specification for the navigation system
- **Attributes**:
  - id: Unique identifier for the navigation goal
  - robot_id: Reference to the HumanoidRobotModel
  - target_position: 3D coordinates of the navigation target
  - target_orientation: Desired orientation at the target
  - navigation_mode: Mode of navigation (WAYPOINT, PATH_FOLLOWING, AVOIDANCE)
  - constraints: Humanoid-specific navigation constraints
  - priority: Priority level of the navigation task
  - status: Current status (PENDING, EXECUTING, COMPLETED, FAILED)
- **Relationships**:
  - Belongs to one HumanoidRobotModel
  - References PerceptionOutput (for environment understanding)
  - Associated with NavigationPath (planned path)

### NavigationPath
- **Description**: Planned path generated by the Nav2 navigation system
- **Attributes**:
  - id: Unique identifier for the path
  - goal_id: Reference to the NavigationGoal
  - waypoints: List of 3D coordinates defining the path
  - path_type: Type of path (GLOBAL, LOCAL, ADAPTED)
  - timestamp: Time when the path was generated
  - path_length: Total length of the path
  - estimated_duration: Estimated time to execute the path
  - collision_free: Boolean indicating if path is collision-free
- **Relationships**:
  - Belongs to one NavigationGoal
  - Associated with RobotState (execution updates)

### RobotState
- **Description**: Current state of the humanoid robot including position, orientation, and joint states
- **Attributes**:
  - id: Unique identifier for the robot state
  - robot_id: Reference to the HumanoidRobotModel
  - position: 3D position in the world coordinate system
  - orientation: Orientation as quaternion
  - joint_states: Map of joint names to positions and velocities
  - velocity: Linear and angular velocity of the robot
  - timestamp: Time when the state was recorded
  - state_type: Type of state (CURRENT, ESTIMATED, PREDICTED)
- **Relationships**:
  - Belongs to one HumanoidRobotModel
  - Associated with NavigationPath (execution updates)
  - Associated with LearningEpisode (for state observations)

### LearningEpisode
- **Description**: Instance of a reinforcement learning episode in the simulation environment
- **Attributes**:
  - id: Unique identifier for the learning episode
  - robot_id: Reference to the HumanoidRobotModel
  - environment_id: Reference to the SimulationEnvironment
  - episode_number: Sequential number of the episode
  - start_time: Timestamp when the episode started
  - end_time: Timestamp when the episode ended
  - total_reward: Cumulative reward for the episode
  - terminal_state: Whether the episode ended in a terminal state
  - success: Boolean indicating if the episode was successful
- **Relationships**:
  - Belongs to one HumanoidRobotModel
  - Belongs to one SimulationEnvironment
  - Has many StateActionReward instances
  - Associated with Policy (current policy used)

### StateActionReward
- **Description**: Tuple representing a state-action-reward transition in RL
- **Attributes**:
  - id: Unique identifier for the SAR instance
  - episode_id: Reference to the LearningEpisode
  - state: Robot state at time t
  - action: Action taken by the robot
  - reward: Reward received after taking the action
  - next_state: Robot state at time t+1
  - terminal: Boolean indicating if this leads to terminal state
  - timestamp: Time when the transition occurred
- **Relationships**:
  - Belongs to one LearningEpisode
  - References RobotState (current and next state)

### SimulationEnvironment
- **Description**: Virtual environment in Isaac Sim where the robot operates
- **Attributes**:
  - id: Unique identifier for the environment
  - name: Descriptive name of the environment
  - description: Detailed description of the environment
  - urdf_path: Path to environment URDF/model files
  - lighting_conditions: Parameters for environment lighting
  - physics_properties: Gravity, friction, and other physics parameters
  - randomization_settings: Configuration for environment randomization
- **Relationships**:
  - Associated with multiple HumanoidRobotModel instances
  - Associated with multiple LearningEpisode instances

### Policy
- **Description**: Reinforcement learning policy that maps states to actions
- **Attributes**:
  - id: Unique identifier for the policy
  - name: Descriptive name of the policy
  - algorithm: RL algorithm used (PPO, DDPG, SAC, etc.)
  - architecture: Neural network architecture description
  - parameters: Serialized model parameters
  - creation_time: Timestamp when policy was created
  - performance_metrics: Evaluation metrics for the policy
  - is_trained: Boolean indicating if the policy is trained
- **Relationships**:
  - Associated with multiple LearningEpisode instances
  - Associated with StateActionReward instances (training data)

## State Transitions

### NavigationGoal States
- PENDING → EXECUTING: When navigation begins
- EXECUTING → COMPLETED: When robot reaches target
- EXECUTING → FAILED: When navigation fails (obstacle, timeout, etc.)
- EXECUTING → PENDING: When goal is modified during execution

### LearningEpisode States
- INITIALIZING → RUNNING: When episode starts
- RUNNING → TERMINATED: When terminal state reached
- RUNNING → INTERRUPTED: When episode is manually stopped
- RUNNING → COMPLETED: When goal is achieved

## Validation Rules

### HumanoidRobotModel
- URDF path must exist and be valid
- Joint configurations must be within physical limits
- Sensor configurations must match actual robot sensors

### SensorData
- Timestamp must be within simulation time bounds
- Data payload format must match sensor type
- Frame ID must correspond to a valid coordinate frame

### NavigationGoal
- Target position must be within environment bounds
- Constraints must be compatible with robot kinematics
- Status transitions must follow defined state machine

### RobotState
- Position and orientation must be within physical constraints
- Joint positions must be within joint limits
- Velocity values must be physically plausible

### LearningEpisode
- Episode duration must be within configured limits
- Reward values must be within defined bounds
- State transitions must follow Markov property