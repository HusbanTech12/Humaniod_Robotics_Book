{
  "permissions": {
    "allow": [
      "Bash(npx docusaurus start --verbose)",
      "Bash(npx docusaurus start)",
      "Bash(npm run build)",
      "Bash(timeout 30 npx docusaurus start)",
      "Bash(npm install)",
      "Bash(git fetch --all --prune)",
      "Bash(.specify/scripts/bash/create-new-feature.sh --json \"/sp.specify Module 2: The Digital Twin (Gazebo & Unity)\n\nTarget audience:\nStudents, researchers, and engineers with foundational knowledge in AI and robotics, looking to simulate humanoid robots in realistic virtual environments. Audience is expected to be familiar with ROS 2 basics and Python programming.\n\nFocus and theme:\nCreating high-fidelity digital twins of humanoid robots and their environments for testing, validation, and human-robot interaction. Emphasis on physics-accurate simulation, sensor emulation, and environment visualization.\n\nGoal:\nEnable learners to design, simulate, and validate humanoid robots in virtual environments using Gazebo and Unity, integrating physics, sensors, and realistic interactions.\n\nLearning objectives:\n\nMaster Gazebo physics simulation: gravity, collisions, and dynamic interactions.\n\nBuild and configure humanoid digital twins in Gazebo using URDF/SDF models.\n\nSimulate sensor data for LiDAR, depth cameras, IMUs, and force/torque sensors.\n\nCreate interactive Unity environments for visualization and human-robot interaction.\n\nIntegrate sensor data streams into ROS 2 topics for perception and control pipelines.\n\nValidate simulation accuracy and correspondence with expected physical behaviors.\n\nSuccess criteria:\n\nFully simulated humanoid robot in Gazebo, with correct joint dynamics and collision responses.\n\nSensors produce realistic and testable data streams compatible with ROS 2.\n\nUnity environment accurately visualizes humanoid actions and supports user interaction.\n\nSensor fusion pipelines integrate simulated perception data with ROS 2 nodes for robot decision-making.\n\nDocumentation and diagrams enable reproducibility by other developers.\n\nConstraints:\n\nUse Gazebo 11 or later and Unity 2021+ versions compatible with ROS 2.\n\nFocus on humanoid robots and human-centered interactions; avoid unrelated robotics platforms.\n\nExclude VR/AR hardware-specific implementations; focus on simulation software only.\n\nMinimum 30% of examples must include integrated sensor pipelines.\n\nNot building:\n\nDetailed game mechanics or unrelated Unity scripting.\n\nHardware deployment outside simulated environments.\n\nFull multi-robot swarm simulations (focus on single humanoid robot).\n\nTechnical details:\n\nResearch-concurrent approach: Study Gazebo physics, URDF/SDF modeling, and Unity integration while authoring content.\n\nInclude example ROS 2 integration code snippets and simulation launch files.\n\nEnsure high-quality diagrams, screenshots, and environment layouts.\n\nCitation style: APA; include Gazebo/Unity documentation and peer-reviewed robotics research.\n\nTimeline and word count:\n\nWord count: 4,500‚Äì6,000 words for this module.\n\nTimeline: Complete module content within 1‚Äì1.5 weeks concurrent with simulation setup.\" --number 1 --short-name \"digital-twin\")",
      "Bash(.specify/scripts/bash/setup-plan.sh --json)",
      "Bash(.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks)",
      "Bash(git add .)",
      "Bash(git merge 001-vla --allow-unrelated-histories -m \"Merge deployment configurations from 001-vla branch\")",
      "Bash(git add:*)",
      "Bash(git commit -m \"chore: sync repository, standardize structure, and prepare for deployment\")",
      "Bash(mkdir -p docs-site/docs/module-2-digital-twin)",
      "Bash(timeout 60 npx docusaurus build)",
      "Bash(cat /tmp/claude/tasks/bfce8b3.output)",
      "Bash(npx tsc --noEmit)",
      "Bash(cat /tmp/claude/tasks/bcec10b.output)",
      "Bash(npx docusaurus start --port 3000)",
      "Bash(cat /tmp/claude/tasks/b27ab70.output)",
      "Bash(ls -d specs/*-isaac-ai-brain)",
      "Bash(.specify/scripts/bash/create-new-feature.sh --json \"/sp.specify Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)\nTarget audience\n\nAdvanced students, AI engineers, and robotics developers with prior experience in ROS 2, robot simulation (Gazebo or Isaac Sim), perception pipelines, and foundational machine-learning concepts. Readers are expected to understand robotic middleware, sensor data flows, and basic navigation principles.\n\nFocus and theme\n\nAdvanced perception, navigation, and intelligence for humanoid robots using the NVIDIA Isaac ecosystem. This module focuses on constructing the \"\"brain\"\" of a physical AI system‚Äîresponsible for perception, localization, planning, and learning‚Äîwhile remaining tightly integrated with ROS 2 and simulation environments.\n\nGoal\n\nEnable learners to design and deploy an AI-robot brain capable of:\n\nPerceiving the environment through multimodal sensors\n\nLocalizing and mapping using visual data\n\nPlanning safe, goal-directed motion\n\nLearning behaviors in simulation\n\nPreparing systems for sim-to-real transfer\n\nusing NVIDIA Isaac Sim, Isaac ROS, and Nav2 as the core technology stack.\n\nLearning objectives\n\nUnderstand the role of NVIDIA Isaac in Physical AI systems\n\nUse Isaac Sim for photorealistic simulation and synthetic data generation\n\nBuild perception pipelines using Isaac ROS\n\nImplement Visual SLAM (VSLAM) for localization and mapping\n\nApply Nav2 for humanoid navigation and path planning\n\nIntroduce reinforcement learning concepts for humanoid control\n\nUnderstand sim-to-real transfer challenges and mitigation strategies\n\nSuccess criteria\n\nIsaac Sim environment successfully runs a humanoid robot with realistic physics and rendering\n\nSynthetic datasets are generated and suitable for perception model validation\n\nIsaac ROS pipelines perform real-time perception and VSLAM\n\nNav2 produces valid, collision-free paths in dynamic environments\n\nLearned policies demonstrate improvement through simulation training\n\nSystem architecture is reproducible and compatible with ROS 2 standards\n\nScope and content coverage\n\n1. The AI-Robot Brain Concept\n\nIntelligence layers in humanoid robots\n\nPerception ‚Üí planning ‚Üí action loop\n\n2. NVIDIA Isaac Platform Overview\n\nIsaac Sim, Isaac ROS, and ecosystem architecture\n\nRole of GPU-accelerated robotics\n\n3. Isaac Sim\n\nPhotorealistic simulation\n\nEnvironment randomization\n\nSynthetic data generation for vision tasks\n\n4. Perception with Isaac ROS\n\nHardware-accelerated perception nodes\n\nSensor fusion (camera, LiDAR, IMU)\n\nVisual SLAM pipelines\n\n5. Navigation and Planning\n\nNav2 architecture and behavior trees\n\nPath planning for humanoid robots\n\nObstacle avoidance in dynamic environments\n\n6. Learning-Based Control\n\nReinforcement learning for robotics\n\nTraining locomotion and manipulation policies in simulation\n\n7. Sim-to-Real Transfer\n\nDomain randomization\n\nNoise modeling\n\nPerformance validation strategies\n\nConstraints\n\nFocus exclusively on NVIDIA Isaac Sim and Isaac ROS\n\nHumanoid-centric navigation and perception (no wheeled-robot assumptions)\n\nReinforcement learning covered at a practical, implementation-oriented level\n\nIntegration must follow ROS 2 communication patterns\n\nNot building\n\nLow-level GPU or CUDA programming\n\nNon-NVIDIA simulators or robotics stacks\n\nFull production deployment pipelines\n\nMathematical proofs of SLAM or reinforcement learning algorithms\n\nTechnical details\n\nResearch-concurrent writing approach\n\nROS 2 as the integration backbone\n\nModular separation of:\n\nPerception\n\nNavigation and planning\n\nLearning and control\n\nArchitecture diagrams for perception stacks and planning pipelines\n\nCitation style: APA (NVIDIA documentation and peer-reviewed robotics research)\n\nTimeline and word count\n\nWord count: 5,000‚Äì6,500 words\n\nTimeline: 1.5‚Äì2 weeks\n\nExecution boundary (IMPORTANT)\n\nThis specification does NOT authorize:\n\n/sp.plan\n\n/sp.tasks\n\n/sp.implement\n\nFurther execution steps require explicit my approval.\" --number 1 --short-name \"isaac-ai-brain\")",
      "WebSearch",
      "mcp__context7__resolve-library-id",
      "mcp__context7__get-library-docs",
      "Bash(git rev-parse --git-dir)",
      "Bash(ls -d specs/*-vla)",
      "Bash(.specify/scripts/bash/create-new-feature.sh --json \"/sp.specify Module 4: Vision-Language-Action (VLA)\nTarget audience\n\nAdvancedatial context\n\nExecutes multi-step behaviors through ROS 2 control systems\n\nThe module culminates in a fully autonomous, conversational humanoid capstone system operating end-to-end in simulation.\n\nLearning objectives\n\nUnderstand the Vision-Language-Action paradigm within embodied AI\n\nDesign voice-to-text pipelines using OpenAI Whisper\n\nConvert natural language instructions into structured action plans\n\nUse LLMs for cognitive planning and task decomposition\n\nIntegrate vision pipelines for object recognition and scene grounding\n\nExecute multi-step robotic behaviors via ROS 2 actions and services\n\nApply safety constraints to ensure predictable and interpretable robot behavior\n\nSuccess criteria\n\nVoice commands are accurately transcribed and normalized\n\nLLM outputs consistently generate valid, structured action plans\n\nVision pipelines correctly identify and localize task-relevant objects\n\nNavigation and manipulation actions execute reliably in simulation\n\nThe Vision-Language-Action loop runs end-to-end without manual intervention\n\nThe capstone humanoid completes complex, multi-step tasks from a single command\n\nScope and content coverage\n\n1. Vision-Language-Action Foundations\n\nFrom command-based robotics to cognitive humanoids\n\nRole of VLA in Physical AI and human-centered robotics\n\n2. Voice-to-Language Processing\n\nSpeech recognition using OpenAI Whisper\n\nCommand normalization and intent extraction\n\n3. Cognitive Planning with LLMs\n\nHigh-level goal interpretation\n\nTask decomposition (e.g., ‚ÄúClean the room‚Äù)\n\nTranslation of language into symbolic or JSON-based action plans\n\nPrompt design for constrained, deterministic outputs\n\n4. Vision Grounding\n\nObject detection and localization\n\nScene understanding and spatial reasoning\n\nLinking perception outputs to planning and execution\n\n5. Action Execution Layer\n\nMapping structured plans to ROS 2 actions, services, and topics\n\nSequencing, synchronization, and state tracking\n\nError handling and recovery behaviors\n\n6. Safety, Reliability, and Control\n\nConstraining LLM outputs for physical systems\n\nGuardrails between reasoning and execution layers\n\nHuman-in-the-loop design considerations\n\n7. Capstone Project: The Autonomous Humanoid\n\nVoice-command initiation\n\nAutonomous navigation with obstacle avoidance\n\nVision-based object identification and manipulation\n\nFully integrated Vision-Language-Action execution in simulation\n\nConstraints\n\nFocus exclusively on simulated humanoid robots (Gazebo / Isaac Sim)\n\nLLMs limited to planning and reasoning, not low-level motor control\n\nAll robot actions must be deterministic and executed via ROS 2\n\nMinimum of five multi-step task demonstrations\n\nNot building\n\nCommercial AI assistant or chatbot comparisons\n\nLLM training, fine-tuning, or dataset creation pipelines\n\nReal-world humanoid hardware deployment\n\nNon-robotic conversational AI systems\n\nTechnical details\n\nResearch-concurrent writing approach\n\nROS 2 as the authoritative execution backbone\n\nStrict modular separation of:\n\nPerception\n\nCognitive planning\n\nPhysical execution\n\nArchitecture diagrams illustrating:\n\nVision-Language-Action loops\n\nPlanning pipelines\n\nSystem-level data flow\n\nCitation style: APA (robotics, embodied AI, and LLM research sources)\n\nTimeline and word count\n\nWord count: 5,000‚Äì6,500 words\n\nTimeline: 1.5‚Äì2 weeks\" --number 2 --short-name \"vla\")",
      "Bash(.specify/scripts/bash/check-prerequisites.sh --json)",
      "Bash(npm start)",
      "Bash(timeout 60 npx docusaurus start --port 3000)",
      "Bash(cat server.log)",
      "Bash(curl -s -o /dev/null -w \"%{http_code}\" http://localhost:3000/humanoid-robotic-book/)",
      "Bash(lsof -i :3000)",
      "Bash(node node_modules/@docusaurus/core/bin/docusaurus.js start --port 3000)",
      "Bash(curl -s -o /dev/null -w \"%{http_code}\" http://localhost:3000/)",
      "Bash(netstat -tulpn)",
      "Bash(curl -s -o /dev/null -w \"%{http_code}\" http://localhost:3001/)",
      "Bash(git commit -m \"chore: Add Vercel configuration and .nojekyll file\n\n- Add vercel.json for SPA routing\n- Add .nojekyll file to bypass Jekyll processing on GitHub Pages\n\nCo-authored-by: Claude Sonnet 4.5 <noreply@anthropic.com>\n\nü§ñ Generated with [Claude Code]\\(https://claude.com/claude-code\\)\")"
    ],
    "deny": [],
    "ask": []
  }
}
